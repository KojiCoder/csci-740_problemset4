\documentclass[12pt]{article}
%\usepackage{palatino, amsfonts, amsmath, fancyheadings}
\usepackage{amsfonts, amsmath, fancyhdr,lastpage,multirow}
\usepackage{graphicx}
\pagestyle{fancy}

\usepackage[noend]{algpseudocode}
\usepackage{algorithm}


%% For marking tables

\usepackage{tikz}
\usetikzlibrary{fit,shapes.misc}

\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-a) at (0,1.5ex) {};%
}
\newcommand\markbottomright[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-#1-b) at (0,0) {};%
    \tikz[overlay,remember picture,thick,dashed,inner sep=3pt]
        \node[draw, rectangle,fit=(marker-#1-a.center) (marker-#1-b.center)] {};%
}
%% End marking tables


\topmargin = -0.8 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\textheight = 9 in
\textwidth = 6.75 in

\newtheorem{theorem}{Theorem}   
\newtheorem{result}{Result}
\newtheorem{definition}{Defintion}  
\newcounter{problem}
\newcounter{solution}
\def\bibite{$\div \hskip-0.77em \times$}
\def\endex{\bibite \kern-3pt \bibite \kern-3pt \bibite}
\def\theproblem{\arabic{problem}}
\newenvironment{problem}
     {\refstepcounter{problem}%
    \medskip\noindent{\bfseries Problem} {\bfseries \theproblem}
      }
%   {\vskip-1.5em  \hphantom{a} \hfill \endex
%     \smallskip}

\newenvironment{solution}
     { \refstepcounter{solution}%
    \medskip\noindent{ \bfseries Solution to Problem {\thesolution}:\\}  }
     {
%     \vskip-1.5em  \hphantom{a} \hfill \endex
      \medskip
       }
\newcommand{\res}[1]{Result~\ref{res:#1}}
\newcommand{\eq}[1]{(\ref{eq:#1})}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\alg}[1]{Algorithm \ref{alg:#1}}
\renewcommand{\labelenumi}{(\alph{enumi})}

% Felisa's Shorthand notations
\def\points#1{{\bfseries [{#1} points]}\\%
}
\def\marks#1{{\bfseries [{#1} points]}}
\font\cmss=cmss10 scaled 1100
\font\goth=eufm10 at 11pt
\def\ds{\displaystyle}
\def\Field{{\hbox{\goth F}}}
\def\Filtration{{\mathbb F}}
\def\borel{{\cal B}}
\def\Real{{\mathbb R}}   % Real numbers
\def\Int{\mathbb N}  % Integer numbers
\def\Prob{{\mathbb P}}
\def\Esp{{\mathbb E}}
\def\prob{\hbox{\cmss P}}
\def\esp{\hbox{\cmss E}}   
\def\var{\hbox{\cmss Var}}   
\def\cov{\hbox{\cmss Cov}}   
\def\eqdist{{\,\buildrel \cal L\over = }\,}    
\def\todist{{\,\buildrel \cal L\over \Longrightarrow }\,}    
\def\eqdef{{\,\buildrel {\rm def} \over = }\,}
%
\def\normal{{\cal N}}  
\def\th{{\theta}}
\def\Th{{\Theta}}
\def\la{{\lambda}}
\def\ind#1{{\mathbf 1}_{\{#1\}}}
\def\given {\,|\,}
\def\order{{\cal O}}
\def\GG{{\cal G}}
\def\loss{{\cal L}}
\def\eff{{\cal E}}
\def\euro{ \hbox{\small{\hskip-.3em$\subset$ \hskip - 1.15 em  $-$}}}
\def\ep{{\epsilon}}
\newcommand\pfrac[2]{{\left(\frac{#1}{#2}\right)}}



\begin{document}\sf

\lhead{ } \chead{ } \rhead{ } \lfoot{ } \cfoot{\sf -- page
\thepage~  of  \pageref{LastPage}  --  } \rfoot{ }                                                     %            <=====   PAGE NUMBERING

\title{\bfseries Computer Modeling  and  Simulation}
\author{
% \sc CSc 86030, Graduate Center and STAT 702, Hunter College, CUNY
}
\date{}

\maketitle

%\null \vskip - 5 cm

\begin{center}
\vskip - 2 cm { {\bfseries Problem Set 4}}
\end{center}

\begin{problem}
Let $P^{(i)}, i = 1,2$ be the transition matrices of two ergodic Markov chains with respective limit probabilities $\pi^{(i)}$, for $i=1,2$. 
\begin{enumerate}
\item Let $X_0=1$ be given. If the result of a coin toss is Heads (H) then $\{X_n\}$ will follow the dynamics given by $P^{(1)}$. Otherwise it will follow those given by $P^{(2)}$. Is $\{X_n\}$ a Markov chain? Determine the transition probabilities. Letting  $p=\Prob(H)$, calculate the limiting probabilities: $\lim_{n\to\infty} \Prob(X_n=i)$.

\item Now suppose that at each step we first throw a coin. If it results $H$ then the following state is chosen according to $P^{(1)}$, otherwise it is chosen according to $P^{(2)}$. Is $\{X_n\}$ a Markov chain? Show  by counterexample that the limit probabilities are not the same as in (a) above.
\end{enumerate}
\end{problem}

\begin{solution}
    \begin{enumerate}
        % part a
        \item $\{X_n\}$ is not a Markov chain because the result of coin flipping is from the past,and it depends on a random coin flipping at time 0. \\

            The transition probabilities:


            \[
            P(X_{n+1} = (j, Coin) \mid X_n = (i, Coin)) =
            \begin{cases}
            P^{(1)}_{ij}, & \text{if } Coin = Head \\
            P^{(2)}_{ij}, & \text{if } Coin = tail
            \end{cases}
            \]
            The limiting probabilities: 

            \[
            \lim_{n \to \infty} \mathbb{P}(X_n = i) = p \, \pi^{(1)}_i + (1 - p) \, \pi^{(2)}_i
            \]
            % part b
            \item $\{X_n\}$ is a Markov chain because the next state depends on current state $\{X_n\}$,and the coin flipping updates each step.

    The transition probabilities:
    \[
    P(X_{n+1} = j \mid X_n = i) = p \cdot P^{(1)}_{ij} + (1 - p) \cdot P^{(2)}_{ij}
    \]
    The limiting probabilities: 

    \[
    \lim_{n \to \infty} \mathbb{P}(X_n = i) = \pi_i
    \]

    It is the stationary distribution of \[ P =p \cdot P^{(1)} + (1 - p) \cdot P^{(2)}\]
    Counter example:
    \[
    P^{(1)} =
    \begin{bmatrix}
    0.2 & 0.8 \\
    0.1 & 0.9
    \end{bmatrix},
    \qquad
    P^{(2)} =
    \begin{bmatrix}
    0.1 & 0.9 \\
    0.5 & 0.5
    \end{bmatrix},
    \qquad
    p = 0.5.
    \]
    Set \( \pi^{(1)} = [x,\ 1 - x] \):

    \[0.2x + 0.1(1 - x) = x
    \Rightarrow  x = \frac{1}{9}
    \]
    \[\pi^{(1)} = \left[\frac{1}{9}, \frac{8}{9}\right]\]
    Set \( \pi^{(2)} = [x,\ 1 - x] \):
    \[0.1x + 0.5(1 - x) = x
    \Rightarrow  x = \frac{5}{14}
    \]
    \[\pi^{(2)} = \left[\frac{5}{14}, \frac{9}{14}\right]\]
    For problem 1a:
    \[
    \pi^{(1a)} = 0.5 \cdot \left[\frac{1}{9}, \frac{8}{9}\right] + 0.5 \cdot \left[\frac{5}{14}, \frac{9}{14}\right]
    = \left[\frac{59}{252}, \frac{193}{252}\right].
    \]
    For problem 1b:
    \[
    P = 0.5 P^{(1)} + 0.5 P^{(2)} =
    \begin{bmatrix}
    0.15 & 0.85 \\
    0.3 & 0.7
    \end{bmatrix}.
    \]
    Set \( \pi^{(1b)} = [x,\ 1 - x] \):
    \[
    0.15x + 0.3(1 - x) = x
    \Rightarrow x = \frac{6}{23}\] 
    \[\pi^{(1b)} = \left[\frac{6}{23}, \frac{17}{23}\right]
    \]
    Therefore, \( \pi^{(1a)} \ne \pi^{(1b)} \)

    \end{enumerate}
\end{solution}
\pagebreak

%\begin{problem} \marks{10}
%Consider a population of individuals each of whom possesses two genes that can be either type $A$ or type $a$. Suppose that in outward appearance (the {\em phenotype}) type $A$ is dominant and type $a$ recessive. Suppose that the population has stabilized, and the percentages of individuals having respective gene pairs $AA$, $aa$ and $Aa$ are $p,q,$ and $r$ Call an individual dominant or recessive depending on phenotype. Let $S_{11}$ denote the probability that an offspring of two dominant parents will be recessive, $S_{10}$ the probability that the offspring of one recessive and one dominant parent will be recessive. Compute $S_{11}$ and $S_{10}$ to show that $S_{11}= S_{10}^2$. These quantities are known in genetics as {\em Snyder's ratios}. 
%\end{problem}

\begin{problem}
An algorithm is built to find the zeroes of a function. If this algorithm is in state $j$ at the $n$-th step, then the probability of finding a zero in the next step is $1/j$. Otherwise its state will be $k \in \{1,2,\ldots, j-1\}$ with probability $2k/j^2$. 
\begin{enumerate}
\item Find the mean number of iterations of the algorithm when we start at state $m$ and show that this is $O(\ln(m))$. 
\item Simulate this process as a Markov chain and estimate the number of iterations until the algorithm finds the zero. Explain how you write the code  in order to achieve a  relative error of $5\%$ in your final estimate.  Verify that your simulation results are consistent with your theoretical answer above. 
\end{enumerate}
\end{problem}

\begin{solution}
    \begin{enumerate}
        % part a
        \item \[P(find \,\, zero)=\frac {1}{j}\]
            \[P(j \to k\,\, |\ not \,\, find \,\, zero )=\frac {2k}{j^2}\].

            There are two cases:\\
            Case 1: Find zero with $P=\frac {1}{j}$,which takes 1 step to get zero.Therefore,
            \[E(j_1)=\frac {1}{j} \cdot 1=\frac{1}{j}\]
            Case 2: we can not find zero and move to k states:
            \[
            P(j \to k) = \frac{2k}{j^2}.
            \]
            Therefore, we need to take $E_k$ more steps :
            \[
            E(j_2)= \sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot (1+E_k)
            \]
            We need take 1 step to get next state, so there is $(1+E_k)$ in $E(j_2)$ function.\\\\
            \[
            E(j)=E(j_1)+E(j_2)=\frac {1}{j}+ \sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot (1+E_k)
                            =\frac {1}{j}+ \sum_{k=1}^{j-1} \frac {2k}{j^2} +\sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot E_k
            \]
            \[
            E(j)=\frac {1}{j}+ \frac {j-1}{j}+\sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot E_k=1+\sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot E_k
            \]
            Therefore,
            \[
            E(m)=1+\sum_{k=1}^{m-1} \frac {2k}{m^2} \cdot E_k
            \]
            For finding Big O, we use the substitution method and only consider the upper bound.\\\\
            Assume\[E(k)\leq C\ln k\quad \text{for all } k < j\]
            \[
            E(j)=1+\sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot E(k)
            \]
            \[E(j)\leq1+\sum_{k=1}^{j-1} \frac {2k}{j^2} \cdot C\ln k\]
            \[
            E(j)\leq 1+\frac {2c}{j^2} \int_1^j k \ln k \, dk
            \]
            \[ \int_1^j k \ln k \, dk =\frac{j^2}{2}\ln j - \frac{j^2}{4}+\frac{1}{4}\]
            Therefore,

            \[
            E(j)\leq 1+ \frac {2c}{j^2}(\frac{j^2}{2}\ln j - \frac{j^2}{4}+\frac{1}{4})
            \]
            \[Ej\leq C \ln j +1-\frac {1}{2}C+\frac{C}{2j^2}\]
            When there is a constant C, that makes $1-\frac {1}{2}C+\frac{C}{2j^2}\leq 0$, we can have:
            \[
            E(j)\leq C \ln j \Rightarrow E(j) \leq \mathcal{O}((\ln j))
            \]
            Therefore, when we start at state m, we have:
            \[E(m)\leq \mathcal{O}((\ln m))\]

        % part b
        \item In our simulation of the algorithm, we used the inverse CDF method to determine the state of the next step.
        We set up the simulations with an initial state of 1000, since the simulation ran very quickly and the variance of the iterations from each simulation was high enough to require a good number (usually over 100 repetitions) in order to arrive at our target relative error of $5\%$.\\\\
        In order to achieve a relative error of $5\%$, we run the simulation several times, evaluating our estimation interval at a $95\%$ confidence level. $T_{n-1,1-0.05/2}$.
        To calculate the estimated variance of the sample mean, we sum the samples and the squares of samples, and use the following formula:
        \begin{equation*}
            \operatorname{Var}(\bar{X}) = \frac{1}{n} \cdot \frac{\sum_{i=1}^{n} x_i^2 - \frac{(\sum_{i=1}^{n} x_i)^2}{n}}{n - 1}
        \end{equation*}
        
        Then we calculate half the length of our estimation interval and compare with our desired relative error.
        \begin{equation*}
            T_{n-1,\,1-\frac{0.05}{2}} \; \sqrt{\operatorname{Var}(\bar{X})}
        \end{equation*}

        \begin{figure}[h]
                \begin{center}
                    \includegraphics[scale=0.46]{p2_dist_fig.png}
                    \includegraphics[scale=0.46]{p2_conf_fig.png}
                \end{center}
        \end{figure}
        \begin{figure}[h]
                \begin{center}
                    \includegraphics[scale=0.65]{p2_avg_fig.jpg}
                \end{center}
        \end{figure}

        To verify that the complexity is $\mathcal{O}(\ln(m))$, we plotted the log of the initial state against the average iterations.
    \end{enumerate}
\end{solution}

\pagebreak

\begin{problem}
There are $N$ individuals in a population, some of whom have a certain viral infection that spreads as follows. Contacts between two members of this population occur in accordance with a Poisson process of rate $\la$. When a contact occurs, it is equally likely to involve any of the $N\choose2$ pairs of individuals. If a contact involves an infected and a healthy individual, then with probability $p$ the non-infected one becomes infected. For Model 1 we assume that once infected, an individual remains infected throughout. In Model 2, once infected, an individual remains ill for an exponential amount of time with intensity $\mu$, after which she/he becomes healthy again. We assume no deaths occur, and there is no spontaneous infection. Let $X(t)$ denote the number of infected individuals at time $t$. 
%For model 2 we assume that recovered individuals have life immunity so they never get the infection again. 
\begin{enumerate}
\item Show that $\{X(t), t\ge 0\}$ is a continuous time Markov Chain (CTMC). Specifically, show that it is a Birth and Death process.  For each of the models, specify the classes of the states and determine whether it is an absorbing Markov chain (identify the absorbing states) or an ergodic Markov Chain. Specify for each model the transition rates $q_{ij}$, the aggregate rate $v_1$ and the transition probabilities of the embedded chain $P_{ij}$. 

\item Starting with $i\ge1$ infected individuals, calculate the expected time until all members of the population are infected in Model 1. What is the expected time until absorption for Model 2? (you may use pseudocode to exhibit the solution, if not analytical).

\item Explain how to do a simulation to estimate the time until absorption for the process of Model 2, starting at $X(0)=1$ and perform the simulations. Include confidence intervals and explain how you calculate the variance. [{\it Hint:} Note that you have already calculated the expected value, which should help validate your simulation code]. 

\item Explain how you would modify the model and the simulation code for the following scenario: contagion follows the same model as before, but once an individual is recovered, he/she has life immunity so they cannot get the infection again. 
\end{enumerate}
\end{problem}

\begin{solution}
    \begin{enumerate}
        % part a
        \item $\{X(t), t\ge 0\}$ is a CTMC because each state transitions after an exponential random holding time, and the transitions of the embedded chain are memoryless.
        It is also a Birth and Death process, since it models the population of infected individuals, and states can only transition to adjacent states (i.e. i to i + 1), representing a change in population.
        For model 1, the number of infected individuals can only increase.
        X(t) is a transient state if $1 \le X(t) < N$, and it is an absorbing state if $X(t) = N$.
        For model 2, all states are accessible from all other states, except in the case of X(t) = 0, where no new infections are possible, and it is an absorbing state.
        Therefore, all states that satisfy $X(t) > 0$ are transient states.\\
        For model 1:\\
        $q_{i, i+1} =\lambda p \frac{i\left(N - i\right)}{\binom{N}{2}}$\\
        $q_{i, i-1} =0$\\
        $v_i = q_{i, i+1}$\\
        $P_{i, i+1} = 1$\\\\
        For model 2:\\
        $q_{i, i+1} =\lambda p \frac{i\left(N - i\right)}{\binom{N}{2}}$\\
        $q_{i, i-1} =i\mu$\\
        $v_i = q_{i, i+1} + q_{i, i-1}$.\\
        $P_{i, i+1} = \frac{q_{i, i+1}}{v_i}$\\
        $P_{i, i-1} = \frac{q_{i, i-1}}{v_i}$\\
        % part b
        \item For model 1:\\
        possible pairs for next infection:
        \[i(N-i)\]
        Total pairs:
        \[\binom{N}{2} = \frac{N(N - 1)}{2}\]
        The transition rate of next state infection:
        \[
        \ q_{i, i+1} = \lambda \cdot \frac{i(N - i)}{\binom{N}{2}} \cdot p.
        \]

        \[
        \ q_{i, i+1} = \lambda \cdot \frac{2i(N - i)}{N(N - 1)} \cdot p.
        \]
        The expected time of a member infected :
        \[
        \mathbb{E}[T_k] = \frac{1}{q_{k,k+1}}.
        \]

        The expected time of all members of the population are infected :
        \[
        \mathbb{E}[T_{total}] = \sum_{k=i}^{N-1} \frac{1}{q_{k,k+1}}.
        \]
        For model 2:\\\\
        Infection transition rate :
        \[
        \ q_{i, i+1} = \lambda \cdot \frac{i(N - i)}{\binom{N}{2}} \cdot p.
        \]
        Recovery transition rate :
        \[
          q_{i, i-1} = i \cdot \mu
          \]
        Total transition rate of moving state:
        \[ {1}{q_{i, i-1} + q_{i, i+1}} \]
        The expected time spent in state i before moving to next state:
        \[\frac{1}{q_{i, i-1} + q_{i, i+1}}\]
        Therefore,total expected time until absorption: $\mathbb{E}[T_{total}]$ =time for next move + time after move.\\\\
        \[\mathbb{E}[T_i]= \frac{1}{q_{i, i-1} + q_{i, i+1}}
        + \frac{q_{i, i+1}}{q_{i, i-1} + q_{i, i+1}} T_{(i+1)}
        + \frac{q_{i, i-1}}{q_{i, i-1} + q_{i, i+1}} T_{(i-1)}\]

        \[\mathbb{E}[T_i]= \frac{1}{{\lambda \cdot \frac{2i(N - i)}{N(N - 1)} \cdot p}+i \cdot \mu}+\frac{\lambda \cdot \frac{2i(N - i)}{N(N - 1)} \cdot p} {{{\lambda \cdot \frac{2i(N - i)}{N(N - 1)} \cdot p}+i \cdot \mu}}\cdot \mathbb{E}[T_{i+1}]+\frac{i \cdot \mu}{{{\lambda \cdot \frac{2i(N - i)}{N(N - 1)} \cdot p+i \cdot \mu}}}\cdot \mathbb{E}[T_{i-1}]\]


        % part c
        \item In both models, we note that since contacts occur within the population according to a Poisson process of rate~$\lambda$,
        and any of the $\binom{N}{2}$ pairs are equally likely to be involved,
        contacts between any one specific pair will follow a Poisson process of rate
        $\displaystyle \frac{\lambda}{\binom{N}{2}}$ (Prop.~8).

        We also note that only contacts between one infected individual and one healthy non-immune individual change the state.
        All other contacts never result in infection.

        Of the $\binom{N}{2}$ pairs that can have contact, there are $x y$ pairs where contact can result in infection (with $x$ infected individuals, and y = N - x healthy individuals). When a contact event with one of these pairs occurs, there is an infection with probability $p$.

        The Poisson processes of contacts that do result in infection can be merged to follow a Poisson process of rate $\lambda {p}\cdot \frac{2xy}{(N)(N - 1)}$.


        Therefore, for the sake of increased efficiency, can simulate the inter-arrival times of only the contact events that change the state.
        For model 2, since we must also simulate the recovery of infected individuals, we note that since they remain ill for an exponential amount of time with intensity $\mu$, we can infer from the fact that Poisson processes are mergeable, and that their inter-arrival times follow an exponential distribution, we can also merge the recovery events with each other, and also with the contact events.
        As follows, the inter-arrival times of recovery events will follow an exponential distribution with intensity $\mu \times \text{num\_infected}$.
        We simulate the inter-arrival times of either event happening with an exponential distribution with the following intensity:
        \[
        \left( \lambda {p} \cdot \frac{2xy}{(N)(N - 1)} \right) + \left( \mu \times \text{num\_infected} \right)
        \]
        We can then simulate whether the event was an infection event or recovery event, with the probability of it being an infection event as the following:
        \[
        \frac{
          \lambda {p} \cdot \dfrac{2xy}{(N)(N - 1)}
        }{
          \left( \lambda {p} \cdot \dfrac{2xy}{(N)(N - 1)} \right)
          + \left( \mu \, n_{\text{infected}} \right)
        }
        \]
        After each event, we increment the clock by the inter-arrival time, repeating until we reach an absorption state.
        We can calculate the confidence interval of our simulation at a $95\%$ confidence level with the same method we describe in our answer for Problem 2(b).

        % part d
        \item To add life immunity to the model, we change the state of the Markov chain to be the number of infected individuals, and the number of non-immune healthy individuals.
        To modify our simulation, we update to our new definition of the state, and use the same equations as outlined in part C, with the caveat of y now representing the number of non-immune healthy individuals, rather than being equal to N - x.

    \end{enumerate}
\end{solution}
\pagebreak

\begin{problem} \marks{30}
You have decided to do consultation for modeling, simulation and optimization. You offer various research  services: modeling, statistical analysis, optimization, software development, etc. There are $N$ such research
stages (or ``tasks'') and they always follow a specific order: research of type  $n$ is always followed  by that of type $n-1$, for $N\ge n > 1$. The time (in hours) required to complete stage $n$ follows a distribution $F_n$ of mean $\mu_n$. Potential clients arrive according to a Poisson process of rate $\la$, and you take the job only if you are free at the time of arrival of the client. If you are already working on a problem then you do not take new contracts. Each problem starts at stage $n$ with probability $p_n$ and ends at stage $1$, following all  intermediate stages. Let $X(t)=n$ if at time $t$ you are working on stage $n$ of a problem, and use $X(t)=0$ if at time $t$ you are free, waiting for new contracts.
\begin{enumerate}
\item \marks{5} Under what conditions on $\{F_n\}$ is $X(t)$ a CTMC? Give the rates $v_i$ and transition kernel $P_{ij}$.
\item \marks{10} Suppose that the distributions $F_n, n=1,\ldots , N$ are not memoryless. You assume that you will charge $c$ dollars per hour of work. Specify the regeneration points of the process (see CTMC Lecture Notes) and determine the long term rate of profit.
\item \marks{15} In order to price your services correctly, let us assume that clients are discouraged if $c$ is too big. Specifically, assume that the probability that clients accept your conditions is given by $P_c=(K-c)/K, 0\le c <K$. If $c\ge K$ then $P_c=0$ (no client will hire you). %and define
%\[
%M\eqdef \sum_{n=1}^N p_n \sum_{k=n}^N \mu_k.
%\]
Design a simulation to evaluate the long term profit as a function of $c$. Explain how you can use these simulations to determine the optimal value of $c$. Specify how you build the simulations for different  values of $c$ in order to compare the long term profit rate, and how do you build the confidence intervals. Plus, specify if you are using a linear or binary search to determine the optimal value. For you simulation, use $K=2, \la=2$, $N=5$ and
\[
\mu= (0.3,  0.1, 0.2, 0.3, 0.1)^\top.
\]

\end{enumerate}

\end{problem}

\begin{solution}
    \begin{enumerate}
        % part a
        \item $X(t)$ is a CTMC when $\{F_n\}$ is a continuous memoryless distribution, or an exponential distribution.\\\\
        Rates $v_i$:
        \begin{itemize}
            \item $\lambda$ for $v_0$
            \item $\lambda \cdot p_i$ and $\mu_i$ for $v_i$ when $i > 0$
        \end{itemize}
        \begin{equation*}
            P_{ij} =
            \begin{bmatrix}
                -\lambda & \lambda \cdot p_1 & \lambda \cdot p_2 & \lambda \cdot p_3 & \dots \\
                \frac{1}{\mu_1} & -\frac{1}{\mu_1}   & 0   & 0   & \dots \\
                0 & \frac{1}{\mu_2}  & -\frac{1}{\mu_2}   & 0   & \dots \\
                0 & 0   & \frac{1}{\mu_3}   & -\frac{1}{\mu_3}   & \dots \\
                0 & 0   & 0   & \frac{1}{\mu_4}   & \dots \\

                \vdots & \vdots   & \vdots   & \vdots & \ddots\\
            \end{bmatrix}
        \end{equation*}
        % part b
        \item The long term profit rate can be expressed as the following:
        \[
        \quad c \cdot \frac{T_{\text{working}}}{T_{\text{total}}}
        \]

        \[
        = c \cdot \frac{\text{customers} \times \text{avg. time per customer}}
        {\text{customers} \times \left(\text{avg. time per customer} + \text{avg. time between customers}\right)}
        \]

        \[
        = c \cdot
        \frac{
        \displaystyle \sum_{n=1}^{N}
        \left( \mu_n \cdot \sum_{i=n}^{N} p_i \right)
        }{
        \displaystyle \sum_{n=1}^{N}
        \left( \mu_n \cdot \sum_{i=n}^{N} p_i \right)
        + \frac{1}{\lambda}
        }
        \]
        % part b
        \item In our code, we estimate the long term profit by simulating only the events that result in a change in state.
        For the sake of simplicity, we use $p_n = 0.2$ for n from 1 to 5.
        We use a DES model, where when we are waiting for a customer, we simulate the time till the next accepting customer as an exponential distribution with rate $\lambda P_c$.
        Keeping our assumptions from part (b), we simulate the stage completion times as $2 \cdot \mu_n U_i$, where $U_i$ is a uniform random variable.
        We calculate the precision of our estimate using the regenerative method.
        We outline the method in more detail in our answer for problem 5(a).
        We chose to search the entire space of valid values of c to find a profit rate maximizing value.
        We found that the long term profit rate is the highest in the neighborhood of c = 1.25.
        \begin{figure}[h]
                \begin{center}
                    \includegraphics[scale=0.75]{p4_profit.jpg}
                \end{center}
        \end{figure}
    \end{enumerate}
\end{solution}
\pagebreak

\begin{problem} \marks{20}
Consider a M/GI/1 queue with Poisson arrivals of rate $\la$ and iid service times $\{S_i\}$ with  distribution $\Gamma(3,4)$. The goal is to estimate the stationary average queue length $\th$. For each simulation model, namely the discrete-event based model and the Petri-net model, do the following. 

\noindent [{\it Hint:} You may re-use your code from Problem Set 2. If you write your code carefully you should be able to just change a subroutine and use common modules for both models]. 
\begin{enumerate}
\item  \marks{10} Let $\alpha=0.05$. Use an adaptive algorithm to stop the simulation so that the approximate confidence interval has precision $\ep = 0.1$. Assume that you do not know the mean and variance of the service distribution (that is, your program could run by reading consecutive service times from a file with historical data, or streaming data). Explain your choice of the algorithm to estimate the confidence interval (independent runs, batch means, discarding of ``warm-up'' period, etc). 
\item \marks{5} Show that the total number of iterations in your simulation model using the stopping rule that you have defined is a random stopping time with respect to the simulation process. 
\item \marks{5}  Perform 20 independent simulation runs to estimate the coverage probability for $\th$. Discuss your results. Discuss the results of the two different approaches: DES {\it versus} Petri Nets. 
\end{enumerate}

\end{problem}

\begin{solution}
    \begin{enumerate}
        % part a
        \item We used the simulation code from the previous problem set and modified it with stopping time.
        In both of these simulations, it stops when the confidence interval has precision $\ep < 0.1$. \\
        There were two methods that we considered to do this:
        \begin{itemize}
            \item Batch Mean Method
            \item Regenerative Method
        \end{itemize}
        In order to obtain the best performance using the Batch Mean Method, where $n$ is the length of each batch, the number of batches would be $\sqrt{n}$.
        As such, $n$ would need to increase to reduce the confidence interval. In addition, it would need to spin up new batches and then balance the length of all the batches.
        All of this work seemes unnecessarily complicated and could require a lot of memory and time. \\
        As a result, we decided to go with the regenerative method.
        For the DES model that determines an estimation of the stationary average, we defined that a cycle ends when there are no customers in the queue and that no customer is being served. \\\\
        For each cycle, we calculate: 
        \begin{equation*}
            \int_{T_{k-1}}^{T_k} \text{queue\_length}\ dt
        \end{equation*}
        as the waiting time in the $k$th cycle.\\\\
        Using the sums of the waiting times, sums of the squares of the waiting times, sums of the regeneration times, and sums of the squares of the regeneration times, and the sample mean, we calculate $\sigma_z$.
        We calculate the width of our confidence interval as:
        \begin{equation*}
            \frac{Z_{0.975}\,\sigma_z}{\bar{T}\sqrt{n}}
        \end{equation*}
        and stop when it is less than 0.1. \\\\
        In the case of the POM model, the output is an estimate of the average sojourn time per customer.
        Since we are finding an average per customer rather than per unit time, we can calculate the total sojourn time in each cycle by using the number of customers in each cycle instead of the regeneration times.
        Otherwise, it follows the same method used in the DES model.

        % part b
        \item The total number of iterations the simulation performs is a random stopping time since we calculate our confidence interval based on past observations. 
        The simulation stops when it reaches less than a fixed number.
        \begin{figure}[h!]
            \begin{center}
                \includegraphics[scale=0.8]{p5_code_snippet.png}
            \end{center}
        \end{figure}
        \newline
        % part c
        \item Running 20 DES estimations took a total of 21.26 seconds, while 20 POM estimations took a very similar 22.76 seconds.
        As such, the running time of these approaches are similar to each other.
        We believe that this is due to the additional computational cost of evaluating confidence intervals as it is the biggest contributor to the computational time of both approaches. 
        The distribution of their output $\theta$ estimates are not significantly different.
        For both of these approaches, the coverage was the expected $95\%$.
        \begin{figure}[h]
            \begin{center}
                \includegraphics[scale=0.46]{p5_des_fig.png}
                \includegraphics[scale=0.46]{p5_pom_fig.png}
            \end{center}
        \end{figure}
    \end{enumerate}
\end{solution}

\end{document}